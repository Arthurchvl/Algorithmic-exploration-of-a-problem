{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN :  K-Nearest Neighbors Algorithme (algorthme des K plus proches voisins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme KNN (k-nearest neighbors) est un algorithme d'apprentissage automatique supervisé simple et facile à mettre en œuvre, qui peut être utilisé pour résoudre les problèmes de classification et de régression (qu'est ce qu'une régréssion ? une classification ?).  \n",
    "Un algorithme d'apprentissage automatique supervisé est un algorithme qui s'appuie sur des données d'entrée étiquetées (des couples données-étiquettes).\n",
    "On dispose donc de données d'apprentissage et lors de la prédiction, lorsqu'on rencontre une nouvelle donnée (de test) à prédire, on cherche les K instances d'entraînement les plus proches de cette nouvelle donnée.  \n",
    "On attribue ensuite  à cette donnée la classe (étiquette) la plus courante parmi ces K instances d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ainsi besoin d'une fonction mathématique pour mesurer la distance séparant nos objets (données). On supposera que nos données sont des vecteurs de $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **distance de Minkowski** entre deux points $A = (x_1, x_2, \\dots, x_n)$ et $B = (y_1, y_2, \\dots, y_n) $ dans un espace à $n$ dimensions (pour nous $\\mathbb{R}^n$) est définie par :\n",
    "$$\n",
    "d_p(A, B) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}, \\quad p \\geq 1\n",
    "$$\n",
    "\n",
    "Cas particuliers :\n",
    "- Pour $p = 1$, on obtient la **distance de Manhattan** :\n",
    "  $$ d_1(A, B) = \\sum_{i=1}^{n} |x_i - y_i| $$\n",
    "- Pour $p = 2$, on obtient la **distance Euclidienne** :\n",
    "$$ d_2(A, B) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "- Pour $p \\to \\infty$, on obtient la **distance de Chebyshev** (ou $l_\\infty$) :\n",
    "$$ d_{\\infty}(A, B) = \\max_{1 \\leq i \\leq n} |x_i - y_i| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles propriétés doit vérifier une fonction (espaces de départ et d'arrivée ?) pour être une distance ? Pour $p=1$ vérifiez que ces propriétés sont vérifiées.    \n",
    "Pour \"voir\" les différences entre ces distances, au moins pour les trois cas particuliers, on pourra tracer dans $\\mathbb{R}^2$ la boule unité correspondante (les points du plan qui sont à une distance inférieure ou égale à 1 de l'origine).  \n",
    "On pourra également se poser la question du choix de la distance...  \n",
    "On choisira également $k$ (le nombre de voisins les plus proches à retenir). Là aussi on se posera la question du choix de $k$ et des conséquences (s'il est trop petit, trop grand ?).   \n",
    "On peut alors mettre en oeuvre l'algorithme en suivant les étapes ci-dessous :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Charger les données\n",
    "2. Pour chaque donnée\n",
    "   - Calculer la distance entre la donnée en question et l'enregistrement courant.\n",
    "   - Ajouter la distance et l'indice de l'enregistrement dans une collection.\n",
    "3. Trier la collection par distances croissantes.\n",
    "4. Choisir les $k$ premières entrées de la collection\n",
    "5. Obtenir les étiquettes de ces entrées\n",
    "6.   - S'il s'agit d'une régression, retourner la valeur moyenne des étiquettes\n",
    "     - S'il s'agit d'un classification, retourner la valeur la plus courante parmi les étiquettes des $k$ voisins retenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra, dans un premier temps, pour tester utiliser comme données d'apprentissage des points de $\\mathbb{N}^2$ étiquettés par des lettres de l'alphabet (on pourra se limiter un petit nombre pour les tests, et même les représenter graphiquement).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se posera également la question de la normalisation des données (de gros écarts entre les valeurs, par exemple la taille en mètre et le poids en kilogramme).\n",
    "De même que se passe t-il si nos données sont de grande dimension (que se passe t'il pour la distance des données à notre donnée de test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment apprécier la qualité de notre classification (régréssion) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'en est-il de la compléxité de cet algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Quelles propriétés doit vérifier une fonction (espaces de départ et d'arrivée ?) pour être une distance ? \n",
    "- Non-négativité : ∀x,y ∈ E, d(x,y) ≥ 0 (déjà mentionnée)\n",
    "- Séparation : d(x,y) = 0 ⟺ x = y (manquante)\n",
    "- Symétrie : ∀x,y ∈ E, d(x,y) = d(y,x) (déjà mentionnée)\n",
    "- Inégalité triangulaire : ∀x,y,z ∈ E, d(x,z) ≤ d(x,y) + d(y,z) (manquante)\n",
    "2) Se poser la question du choix de la distance. \n",
    "- Distance euclidienne (p=2) : privilégiée pour les espaces continus où les variations dans toutes les directions ont une importance similaire. Adapté aux données numériques continues.\n",
    "- Distance de Manhattan (p=1) : utile quand les déplacements ne peuvent se faire qu'horizontalement ou verticalement (comme dans une ville). Moins sensible aux valeurs aberrantes que la distance euclidienne.\n",
    "- Distance de Chebyshev (p→∞) : considère uniquement la plus grande différence entre coordonnées. Utile quand une seule dimension peut être critique.\n",
    "3) Se poser la question du choix du k (nombre de voisins à prendre dans le cercle).\n",
    "- k trop petit (ex: k=1) : très sensible au bruit et risque de surapprentissage (overfitting). La classification est basée sur trop peu d'exemples potentiellement non représentatifs.\n",
    "- k trop grand : tendance au sous-apprentissage (underfitting), perd la capacité à capturer des motifs locaux importants et peut introduire un biais lié aux classes majoritaires.\n",
    "- Règle empirique : k ≈ √n où n est le nombre d'échantillons d'entraînement\n",
    "- Considération de parité : pour la classification binaire, choisir k impair évite les égalités.\n",
    "4) Se poser la question de la normalisation des données (écarts importants entre les valeurs).\n",
    "- Min-Max : ramène les valeurs entre 0 et 1\n",
    "- Standardisation : transforme les données pour avoir une moyenne de 0 et un écart-type de 1\n",
    "5) Se poser la question pour les données de grande dimension.\n",
    "- Les distances tendent à devenir uniformes, rendant la notion de \"proximité\" moins pertinente\n",
    "- Le volume de l'espace croît exponentiellement avec la dimension, créant des \"espaces vides\"\n",
    "- Solution possible : réduction de dimension (PCA, t-SNE), sélection de caractéristiques\n",
    "6) Comment apprécier la qualité de notre classification et régréssion.\n",
    "- Une classification est une tâche d’apprentissage supervisé où l’objectif est d’attribuer une étiquette ou une catégorie à une donnée en fonction des caractéristiques observées.\n",
    "- Pour la classification : \n",
    "   - Matrice de confusion : vrai positifs, faux positifs, vrai négatifs, faux négatifs\n",
    "   - Précision, Rappel, F1-score\n",
    "   - Accuracy (pourcentage de bonnes prédictions)\n",
    "   - Validation croisée pour estimer la performance sur des données non vues\n",
    "-  Pour la régression : \n",
    "   - Erreur quadratique moyenne (MSE)\n",
    "   - Erreur absolue moyenne (MAE)\n",
    "   - Coefficient de détermination (R²)\n",
    "7) Quelle est la compléxité de l'algorithme KNN.\n",
    "- Phase d'apprentissage : O(1) car l'algorithme stocke simplement les exemples\n",
    "- Phase de prédiction : - O(n×d + n×log(n)) où :\n",
    "    - n est le nombre d'exemples d'apprentissage\n",
    "    - d est la dimension des données\n",
    "    - O(n×d) pour calculer les distances\n",
    "    - O(n×log(n)) pour trier les distances\n",
    "- Cette complexité rend KNN inefficace pour les grands jeux de données"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
